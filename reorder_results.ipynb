{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT PARAMS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "OUTPUT_DIR = 'models_reordered'\n",
    "if os.path.isdir(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# Models\n",
    "ALL_MODELS = ['pointnet', 'pointnet++', 'so-net', 'kcnet', 'dgcnn', 'pointcnn'] #'deepsets',\n",
    "\n",
    "# Class names\n",
    "CLASSES_COUNT = 40\n",
    "CLASS_NAMES = ['airplane', 'bathtub', 'bed', 'bench', 'bookshelf', 'bottle', 'bowl',\n",
    "               'car', 'chair', 'cone', 'cup', 'curtain', 'desk', 'door', 'dresser',\n",
    "               'flower_pot', 'glass_box', 'guitar', 'keyboard', 'lamp', 'laptop',\n",
    "               'mantel', 'monitor', 'night_stand', 'person', 'piano', 'plant', 'radio',\n",
    "               'range_hood', 'sink', 'sofa', 'stairs', 'stool', 'table', 'tent', 'toilet',\n",
    "               'tv_stand', 'vase', 'wardrobe', 'xbox']\n",
    "\n",
    "# Test class instances\n",
    "CLASS_TEST_INSTANCES = [100, 50, 100, 20, 100, 100, 20, 100, 100, 20, 20, 20, 86, 20,\n",
    "                        86, 20, 100, 100, 20, 20, 20, 100, 100, 86, 20, 100, 100, 20,\n",
    "                        100, 20, 100, 20, 20, 100, 20, 100, 100, 100, 20, 20]\n",
    "\n",
    "# Ensemble params\n",
    "MODELS_COUNT = 10\n",
    "ENSEMBLE_MODELS_NUMBER = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PROBS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = {}\n",
    "true_labels = {}\n",
    "probabilities_reordered = {}\n",
    "true_labels_reordered = {}\n",
    "for model in ALL_MODELS:\n",
    "    \n",
    "    # Load and transpose\n",
    "    model_filepath = os.path.join('models_raw', model)\n",
    "    prob_filepath = os.path.join(model_filepath, 'probabilities.npy')\n",
    "    probabilities[model] = np.load(prob_filepath)\n",
    "    probabilities[model] = np.transpose(probabilities[model], axes=(2, 0, 1))\n",
    "\n",
    "    # probabilities[model] =  softmax(probabilities[model])\n",
    "    probabilities[model] = probabilities[model]/np.std(probabilities[model])\n",
    "\n",
    "    label_filepath = os.path.join(model_filepath, 'true_labels.npy')\n",
    "    true_labels[model] = np.load(label_filepath)\n",
    "    for class_idx in range(CLASSES_COUNT):\n",
    "        error_info = (\"Test cloud instances for model \" + model + \" and class \"\n",
    "                      + str(class_idx) + \" differs! Should be \" + str(CLASS_TEST_INSTANCES[class_idx])\n",
    "                      + \" but got \" + str(np.sum(true_labels[model] == class_idx)))\n",
    "        assert np.sum(true_labels[model] == class_idx) == CLASS_TEST_INSTANCES[class_idx] or \\\n",
    "          (model=='deepsets' and class_idx==12 and np.sum(true_labels[model] == class_idx) == 85) or \\\n",
    "          (model=='deepsets' and class_idx==39 and np.sum(true_labels[model] == class_idx) == 19), \\\n",
    "          error_info\n",
    "                \n",
    "    assert (probabilities[model].shape[0] == ENSEMBLE_MODELS_NUMBER)\n",
    "    assert (probabilities[model].shape[1] == 2468 or (model=='deepsets' and probabilities[model].shape[1] == 2466))\n",
    "    assert (probabilities[model].shape[2] == CLASSES_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'so-net' in ALL_MODELS:\n",
    "    sonet_list_path = 'models_raw/so-net/modelnet40_test.txt'\n",
    "    sonet_list = []\n",
    "    with open(sonet_list_path) as f:\n",
    "        for row in f:\n",
    "            sonet_list.append(row[:-1])\n",
    "    probabilities_reordered['so-net'] = probabilities['so-net']\n",
    "    true_labels_reordered['so-net'] = true_labels['so-net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pointnet' in ALL_MODELS:\n",
    "    pointnet_list_path = \\\n",
    "        ['models_raw/pointnet/ply_data_test_0_id2file.json',\n",
    "         'models_raw/pointnet/ply_data_test_1_id2file.json']\n",
    "    pointnet_list = []\n",
    "    for filepath in pointnet_list_path:\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "            pointnet_list += data\n",
    "    pointnet_list = [(i, pointnet_list[i].split('/')[1][:-4]) for i in range(len(pointnet_list))]\n",
    "    pointnet_list = sorted(pointnet_list, key=lambda x: x[1])\n",
    "\n",
    "    # Size assert wrt SO-Net\n",
    "    assert(len(pointnet_list) == len(sonet_list))\n",
    "    for i in range(len(sonet_list)):\n",
    "        assert(sonet_list[i] == pointnet_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PointNet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pointnet++' in ALL_MODELS:\n",
    "    pointnetpp_list_path = \\\n",
    "        ['models_raw/pointnet++/ply_data_test_0_id2file.json',\n",
    "         'models_raw/pointnet++/ply_data_test_1_id2file.json']\n",
    "    pointnetpp_list = []\n",
    "    for filepath in pointnetpp_list_path:\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "            pointnetpp_list += data\n",
    "    pointnetpp_list = [(i, pointnetpp_list[i].split('/')[1][:-4]) for i in range(len(pointnetpp_list))]\n",
    "    pointnetpp_list = sorted(pointnetpp_list, key=lambda x: x[1])\n",
    "\n",
    "    # Size assert wrt SO-Net\n",
    "    assert(len(pointnetpp_list) == len(sonet_list))\n",
    "    for i in range(len(sonet_list)):\n",
    "        assert(sonet_list[i] == pointnetpp_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dgcnn' in ALL_MODELS:\n",
    "    dgcnn_list_path = \\\n",
    "        ['models_raw/dgcnn/ply_data_test_0_id2file.json',\n",
    "         'models_raw/dgcnn/ply_data_test_1_id2file.json']\n",
    "    dgcnn_list = []\n",
    "    for filepath in dgcnn_list_path:\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "            dgcnn_list += data\n",
    "    dgcnn_list = [(i, dgcnn_list[i].split('/')[1][:-4]) for i in range(len(dgcnn_list))]\n",
    "    dgcnn_list = sorted(dgcnn_list, key=lambda x: x[1])\n",
    "\n",
    "    # Size assert wrt SO-Net\n",
    "    assert(len(dgcnn_list) == len(sonet_list))\n",
    "    for i in range(len(sonet_list)):\n",
    "        assert(sonet_list[i] == dgcnn_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pointcnn' in ALL_MODELS:\n",
    "    pointcnn_list_path = \\\n",
    "        ['models_raw/pointcnn/ply_data_test_0_id2file.json',\n",
    "         'models_raw/pointcnn/ply_data_test_1_id2file.json']\n",
    "    pointcnn_list = []\n",
    "    for filepath in pointcnn_list_path:\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "            pointcnn_list += data\n",
    "    pointcnn_list = [(i, pointcnn_list[i].split('/')[1][:-4]) for i in range(len(pointcnn_list))]\n",
    "    pointcnn_list = sorted(pointcnn_list, key=lambda x: x[1])\n",
    "\n",
    "    # Size assert wrt SO-Net\n",
    "    assert(len(pointcnn_list) == len(sonet_list))\n",
    "    for i in range(len(sonet_list)):\n",
    "        assert(sonet_list[i] == pointcnn_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearange pointnet/pointnet++/dgcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder pointnet\n",
    "if 'pointnet' in ALL_MODELS:\n",
    "    pointnet_indices = [x[0] for x in pointnet_list]\n",
    "    probabilities_reordered['pointnet'] = probabilities['pointnet'][:, pointnet_indices, :]\n",
    "    true_labels_reordered['pointnet'] = true_labels['pointnet'][pointnet_indices]\n",
    "    assert((true_labels_reordered['pointnet'] == true_labels_reordered['so-net']).all())\n",
    "\n",
    "# Reorder pointnet++\n",
    "if 'pointnet++' in ALL_MODELS:\n",
    "    pointnetpp_indices = [x[0] for x in pointnetpp_list]\n",
    "    probabilities_reordered['pointnet++'] = probabilities['pointnet++'][:, pointnetpp_indices, :]\n",
    "    true_labels_reordered['pointnet++'] = true_labels['pointnet++'][pointnetpp_indices]\n",
    "    assert((true_labels_reordered['pointnet++'] == true_labels_reordered['so-net']).all())\n",
    "\n",
    "# Reorder dgcnn\n",
    "if 'dgcnn' in ALL_MODELS:\n",
    "    dgcnn_indices = [x[0] for x in dgcnn_list]\n",
    "    probabilities_reordered['dgcnn'] = probabilities['dgcnn'][:, dgcnn_indices, :]\n",
    "    true_labels_reordered['dgcnn'] = true_labels['dgcnn'][dgcnn_indices]\n",
    "    assert((true_labels_reordered['dgcnn'] == true_labels_reordered['so-net']).all())\n",
    "\n",
    "# Reorder pointcnn\n",
    "if 'pointcnn' in ALL_MODELS:\n",
    "    pointcnn_indices = [x[0] for x in pointcnn_list]\n",
    "    probabilities_reordered['pointcnn'] = probabilities['pointcnn'][:, pointcnn_indices, :]\n",
    "    true_labels_reordered['pointcnn'] = true_labels['pointcnn'][pointcnn_indices]\n",
    "    assert((true_labels_reordered['pointcnn'] == true_labels_reordered['so-net']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata_pointnet = np.load('models_raw/pointnet/xdata.npy')[pointnet_indices, :, :]\n",
    "if 'kcnet' in ALL_MODELS:\n",
    "    xdata_kcnet = np.load('models_raw/kcnet/xdata.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kcnet' in ALL_MODELS:\n",
    "    kcnet_indices = []\n",
    "    for i in range(xdata_pointnet.shape[0]):\n",
    "        sample_pointnet = xdata_pointnet[i, :, :]\n",
    "        hits = 0\n",
    "        for j in range(xdata_kcnet.shape[0]):\n",
    "            sample_kcnet = xdata_kcnet[j, :, :]\n",
    "            if np.equal(sample_pointnet, sample_kcnet).all():\n",
    "                kcnet_indices.append(j)\n",
    "                hits += 1\n",
    "        assert(hits == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kcnet' in ALL_MODELS:\n",
    "    probabilities_reordered['kcnet'] = probabilities['kcnet'][:, kcnet_indices, :]\n",
    "    true_labels_reordered['kcnet'] = true_labels['kcnet'][kcnet_indices]\n",
    "    assert((true_labels_reordered['kcnet'] == true_labels_reordered['so-net']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'deepsets' in ALL_MODELS:\n",
    "    xdata_deepsets = np.load('/home/chechli/ModelNet/DeepSets/DeepSets/PointClouds/xdata.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'deepsets' in ALL_MODELS:\n",
    "    import shutil\n",
    "    matching_basepath = '/home/chechli/ModelNet/DeepSets/matching'\n",
    "    if os.path.exists(matching_basepath):\n",
    "        shutil.rmtree(matching_basepath)\n",
    "    os.mkdir(matching_basepath)\n",
    "\n",
    "    for i in range(CLASSES_COUNT):\n",
    "        class_dir = os.path.join(matching_basepath, 'class_' + str(i))\n",
    "        os.mkdir(class_dir)\n",
    "        pointnet_path = os.path.join(class_dir, 'pointnet')\n",
    "        os.mkdir(pointnet_path)\n",
    "        deepsets_path = os.path.join(class_dir, 'deepsets')\n",
    "        os.mkdir(deepsets_path)\n",
    "\n",
    "        deep_sets_samples_list = []\n",
    "        for k in range(true_labels['deepsets'].shape[0]):\n",
    "            if true_labels['deepsets'][k] == i:\n",
    "                deep_sets_samples_list.append(k)\n",
    "        pointnet_samples_list = []\n",
    "        for k in range(true_labels['pointnet'].shape[0]):\n",
    "            if true_labels['pointnet'][k] == i:\n",
    "                pointnet_samples_list.append(k)\n",
    "\n",
    "        for j in pointnet_samples_list:\n",
    "            sample_pointnet = xdata_pointnet[j, :, :]\n",
    "            sample_path = os.path.join(pointnet_path, 'sample_' + str(j) + '.npy')\n",
    "            np.save(sample_path, sample_pointnet)\n",
    "        for j in deep_sets_samples_list: #xdata_deepsets.shape[0]):\n",
    "            sample_deepsets = xdata_deepsets[j, :, :]\n",
    "            sample_path = os.path.join(deepsets_path, 'sample_' + str(j) + '.npy')\n",
    "            np.save(sample_path, sample_deepsets)\n",
    "        print('Processed class', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in probabilities_reordered:\n",
    "    base_path = os.path.join(OUTPUT_DIR, model_name)\n",
    "    os.mkdir(base_path)\n",
    "    np.save(os.path.join(base_path, 'probabilities.npy'), probabilities_reordered[model_name])\n",
    "    np.save(os.path.join(base_path, 'true_labels.npy'), true_labels_reordered[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
